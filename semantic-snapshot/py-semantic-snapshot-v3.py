# -*- coding: utf-8 -*-
"""
py-semantic-snapshot.py (V3.0)
Python é¡¹ç›®è¯­ä¹‰å¿«ç…§ç”Ÿæˆå™¨ - å®ç°æè‡´ Token å‹ç¼©ä¸æ— æŸç†è§£

ä¸»è¦åŠŸèƒ½æ›´æ–° (V3.0):
1. æ•°æ®å¢å¼ºï¼šè®°å½•å¸¸é‡/å˜é‡/ç±»å±æ€§çš„åˆå§‹å€¼ï¼ˆæˆªæ–­ï¼‰ï¼Œæ”¯æŒDocstringå‚æ•°/è¿”å›ä¿¡æ¯æå–ã€‚
2. ä¸Šä¸‹æ–‡è¿½æº¯ï¼šè®°å½•å†…éƒ¨ import è¯­å¥çš„è¡Œå·ã€‚
3. æ‘˜è¦é‡åŒ–ï¼šæ¨¡å—çº§æ‘˜è¦ (sm) å¢åŠ å¹³å‡ CCN å’Œé«˜ CCN å‡½æ•°è®¡æ•°ã€‚
4. Tokenæ§åˆ¶ï¼šæ–°å¢ CLI å‚æ•°æ§åˆ¶ Docstring å’Œå‚æ•°åˆ—è¡¨çš„æˆªæ–­é•¿åº¦ã€‚
5. é‡‡æ ·åŠŸèƒ½ï¼šå¯¹é«˜ CCN å‡½æ•°è¿›è¡Œä»£ç ç‰‡æ®µé‡‡æ · (sample)ã€‚

ä½¿ç”¨ç¤ºä¾‹ï¼š
    python3.9 py-semantic-snapshot.py ./project -o digest.yaml --max-doc-len 100 --ccn-threshold 10
"""

from __future__ import unicode_literals
import os
import ast
import re
import subprocess
import sys
import argparse
from collections import defaultdict

try:
    import yaml  # éœ€è¦: pip install PyYAML
    import networkx as nx # éœ€è¦: pip install networkx
    import matplotlib
    import matplotlib.pyplot as plt # éœ€è¦: pip install matplotlib
    
    # å…¼å®¹æœåŠ¡å™¨ / æ— æ˜¾ç¤ºç¯å¢ƒ
    matplotlib.use("Agg")
    CAN_GRAPH = True
except ImportError:
    print("âš ï¸ ç¼ºå°‘ä¾èµ–: PyYAML, networkx, æˆ– matplotlibã€‚å›¾è¡¨åŠŸèƒ½å°†è·³è¿‡ã€‚")
    CAN_GRAPH = False


# --- é…ç½®ä¸å·¥å…·å‡½æ•° ---
if sys.version_info[0] == 2:
    text_type = unicode
    string_types = (str, unicode)
else:
    text_type = str
    string_types = (str,)

def ensure_unicode(s):
    if isinstance(s, text_type):
        return s
    if isinstance(s, bytes):
        return s.decode("utf-8", errors="ignore")
    return text_type(s)

def get_gitignored_files(repo_path):
    try:
        ignored = subprocess.check_output(
            ["git", "ls-files", "--others", "-i", "--exclude-standard"],
            cwd=repo_path,
        ).decode("utf-8", errors="ignore").splitlines()
        return set(ignored)
    except Exception:
        return set()
    
# --- å¤æ‚åº¦è®¡ç®—è¾…åŠ©å‡½æ•° ---

def calculate_complexity(node):
    """
    è®¡ç®—ç®€åŒ–ç‰ˆçš„ McCabe åœˆå¤æ‚åº¦ (CCN)ã€‚
    ä¿®è®¢ï¼šCCN ä» 1 å¼€å§‹ï¼Œå¹¶ç¡®ä¿èƒ½å¤Ÿå¤„ç†å‡½æ•°ä½“ï¼ˆä¸€ä¸ª AST èŠ‚ç‚¹åˆ—è¡¨ï¼‰ã€‚
    """
    complexity = 1  # é»˜è®¤å¤æ‚åº¦ä¸º 1 (å‡½æ•°å®šä¹‰æœ¬èº«)
    
    # AST èŠ‚ç‚¹æˆ–èŠ‚ç‚¹åˆ—è¡¨
    nodes_to_walk = node if isinstance(node, (list, tuple)) else [node]
    
    def count_control_flow(n):
        nonlocal complexity
        
        if isinstance(n, (ast.If, ast.For, ast.While, ast.AsyncFor, ast.With, ast.AsyncWith)):
            complexity += 1
        elif isinstance(n, ast.Try):
            # åŸºç¡€ Try (1) + æ¯ä¸ª Except/Else/Finally å—çš„é™„åŠ è·¯å¾„ (è¿™é‡Œåªç®—æ¯ä¸ª except å¤„ç†å™¨)
            complexity += len(n.handlers)
        elif isinstance(n, ast.BoolOp):
            if isinstance(n.op, (ast.And, ast.Or)):
                complexity += len(n.values) - 1
        elif isinstance(n, (ast.ListComp, ast.SetComp, ast.DictComp, ast.GeneratorExp)):
            for generator in n.generators:
                complexity += len(generator.ifs)

    # éå†æ‰€æœ‰èŠ‚ç‚¹ï¼Œå¦‚æœæ˜¯åˆ—è¡¨ï¼Œåˆ™éå†åˆ—è¡¨ä¸­çš„æ¯ä¸ªæ ¹èŠ‚ç‚¹
    for root_node in nodes_to_walk:
        if root_node:
            # æ­£ç¡®ä½¿ç”¨ ast.walk: éå†èŠ‚ç‚¹åŠå…¶æ‰€æœ‰å­èŠ‚ç‚¹
            # æ³¨æ„: å¦‚æœä¼ å…¥çš„æ˜¯å‡½æ•°ä½“åˆ—è¡¨ï¼Œæˆ‘ä»¬å·²ç»åœ¨å¤–å±‚ for å¾ªç¯å¤„ç†äº†åˆ—è¡¨æœ¬èº«
            # æ­¤æ—¶ root_node å°±æ˜¯åˆ—è¡¨ä¸­çš„ä¸€ä¸ªè¯­å¥èŠ‚ç‚¹
            for n in ast.walk(root_node):
                count_control_flow(n) # å¯¹éå†åˆ°çš„æ¯ä¸ªèŠ‚ç‚¹è°ƒç”¨è®¡æ•°å‡½æ•°
    
    # åŸå§‹ä»£ç åœ¨ _process_function_or_method ä¸­ä¼ å…¥çš„æ˜¯ node.body (list)ï¼Œ
    # å› æ­¤å‡½æ•°ä½“åˆ—è¡¨ä¸­çš„èŠ‚ç‚¹éƒ½éœ€è¦è¢«éå†ã€‚
    # ç»è¿‡ä¸Šè¿°ä¿®æ”¹ï¼Œå¦‚æœä¼ å…¥çš„æ˜¯åˆ—è¡¨ï¼Œæˆ‘ä»¬å°†éå†åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ ã€‚
    
    return complexity

# --- AST è¯­ä¹‰æå–å™¨ (V3.0) ---

IO_CALLS = {
    "open": "File_IO", "read": "File_IO", "write": "File_IO",
    "requests.get": "Network_HTTP", "requests.post": "Network_HTTP",
    "socket.socket": "Network_Socket", 
    "db.connect": "Database_Op", "cursor.execute": "Database_Op",
    "subprocess.run": "IPC_Process", "os.popen": "IPC_Process",
}

class PythonSemanticExtractor(ast.NodeVisitor):
    """ä½¿ç”¨ AST æå– Python ä»£ç è¯­ä¹‰ç»“æ„ (V3.0, çŸ­é”®å)"""

    def __init__(self, args):
        self.args = args  # æ¥æ”¶ CLI å‚æ•°
        self.info = {
            "im": [],              # imports (å¸¦è¡Œå·)
            "fim": {},             # from_imports: {module: [items]} (å¸¦è¡Œå·)
            "cl": [],              # classes
            "fn": [],              # functions 
            "md": {},              # methods
            "ds": [],              # docstrings (sampled)
            "cv": [],              # constants/vars (å¸¦å€¼å’Œç±»å‹)
            "cl_attr": [],         # class attributes (æ–°å¢)
            "dc": [],              # decorators (æ¨¡å—çº§)
            "th": [],              # type hints (æ¨¡å—çº§)
            "stat": {
                "async": 0,           
                "th": 0,              
                "io": defaultdict(int),   
                "err": {"total": 0, "generic": 0},  
            },
        }
        self.current_class = None
        # å­˜å‚¨æ–‡ä»¶å†…å®¹è¡Œï¼Œç”¨äºä»£ç ç‰‡æ®µé‡‡æ · (éœ€æ±‚6)
        self.content_lines = [] 
        
    def set_content_lines(self, lines):
        self.content_lines = lines

    # --- Import (æ–°å¢è¡Œå·è¿½è¸ª - éœ€æ±‚2) ---

    def visit_Import(self, node):
        for alias in node.names:
            # è®°å½• import è¯­å¥çš„è¡Œå·
            self.info["im"].append({"n": alias.name, "ln": node.lineno})
        self.generic_visit(node)

    def visit_ImportFrom(self, node):
        module = node.module or ""
        # è®°å½• import è¯­å¥çš„è¡Œå·
        items = [{"n": alias.name, "ln": node.lineno} for alias in node.names]
        self.info["fim"].setdefault(module, []).extend(items)
        self.generic_visit(node)

    # --- Class (æ–°å¢ç±»å±æ€§æå– - éœ€æ±‚3a) ---

    def visit_ClassDef(self, node):
        class_info = {
            "n": node.name,
            "ln": node.lineno, 
            "bs": [self._get_name(base) for base in node.bases], 
            "dc": [self._get_name(dec) for dec in node.decorator_list], 
            "attrs": [] # ç±»å±æ€§åˆ—è¡¨
        }
        
        # Docstring é‡‡æ ·
        docstring = ast.get_docstring(node)
        if docstring and len(docstring) > 10:
            doc_len = min(len(docstring), self.args.max_docstring_len)
            self.info["ds"].append({"t": "cl", "n": node.name, "doc": docstring[:doc_len] + ("..." if doc_len < len(docstring) else "")})

        # éå†ç±»ä½“ï¼Œæå–ç±»å±æ€§ï¼ˆAssignment åœ¨æ–¹æ³•å®šä¹‰å‰å‡ºç°ï¼‰
        for item in node.body:
            if isinstance(item, ast.Assign):
                # Class attributes (V3.0)
                value_repr = self._get_annotation(item.value, max_len=self.args.max_assign_len)
                for target in item.targets:
                    if isinstance(target, ast.Name):
                        name = target.id
                        is_constant = name.isupper() and (name.replace('_', '').isalnum())
                        class_info["attrs"].append({
                            "n": name, 
                            "ln": target.lineno,
                            "const": is_constant,
                            "val": value_repr, 
                        })
            elif isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):
                # é‡åˆ°å‡½æ•°/æ–¹æ³•åï¼Œåœæ­¢æŸ¥æ‰¾ç±»å±æ€§
                break 

        self.info["cl"].append(class_info)

        # è¿›å…¥ç±»ä¸Šä¸‹æ–‡
        old_class = self.current_class
        self.current_class = node.name
        self.generic_visit(node)
        self.current_class = old_class

    # --- Function/Method (æ–°å¢ Docstring å‚æ•°è§£æ/Argæˆªæ–­/ä»£ç é‡‡æ · - éœ€æ±‚3b/4/6) ---

    def _parse_docstring_params(self, docstring):
        """ç®€å•å¯å‘å¼è§£æ Docstring ä¸­çš„å‚æ•°å’Œè¿”å›å€¼ (éœ€æ±‚3b)"""
        if not docstring: return None
        
        parsed = {}
        # åŒ¹é… Google/Sphinx é£æ ¼çš„å‚æ•°å’Œè¿”å›
        param_matches = re.findall(r"^\s*(?:Args|Parameters|:param)\s*:\s*(\w+)\s*(?:(?:[ \t]|\([\w,]+\))?:?\s*(.*))?", docstring, re.MULTILINE | re.IGNORECASE)
        return_matches = re.findall(r"^\s*(?:Returns|:returns:)\s*:\s*(.*)", docstring, re.MULTILINE | re.IGNORECASE)

        if param_matches:
            parsed["p"] = [f"{n}: {d.strip()}" for n, d in param_matches if n]
        if return_matches:
            parsed["r"] = [r.strip() for r in return_matches]
        
        return parsed if parsed else None


    def _process_function_or_method(self, node):
        # ä¼ å…¥ node.body ä»¥è®¡ç®— CCN (å› ä¸º CCN = 1 å·²ç»åœ¨ calculate_complexity ä¸­è®¡ç®—)
        ccn = calculate_complexity(node.body)
        
        # å‚æ•°æˆªæ–­ (éœ€æ±‚4)
        args_list = [arg.arg for arg in node.args.args]
        if len(args_list) > self.args.max_args_len:
            args_list = args_list[:self.args.max_args_len] + ["..."]

        func_info = {
            "n": node.name,
            "ln": node.lineno,
            "cx": ccn, 
            "args": args_list,
            "dc": [self._get_name(dec) for dec in node.decorator_list],
            "ret": self._get_annotation(node.returns),
            "async": isinstance(node, ast.AsyncFunctionDef),
        }

        if func_info["async"]: self.info["stat"]["async"] += 1
        
        # ä»£ç ç‰‡æ®µé‡‡æ · (éœ€æ±‚6)
        if ccn >= self.args.ccn_threshold and self.content_lines:
            start_line = node.lineno
            end_line = node.end_lineno if hasattr(node, 'end_lineno') else start_line + 5
            
            # é‡‡æ · N è¡Œ
            sample_end = min(end_line, start_line + self.args.code_sample_lines)
            # ASTè¡Œå·æ˜¯1-basedï¼Œåˆ—è¡¨æ˜¯0-basedã€‚æˆ‘ä»¬æƒ³è¦åŒ…å« node.lineno è¡Œï¼Œæ‰€ä»¥ä» node.lineno - 1 å¼€å§‹
            # ä½†æ˜¯ä¸ºäº†è·å–å‡½æ•°ä½“çš„è¡Œï¼Œæˆ‘ä»¬ä» start_line å¼€å§‹ï¼ˆå³å®šä¹‰è¡Œï¼‰
            sample_lines = self.content_lines[start_line - 1 : sample_end]
            
            # ç§»é™¤å…¬å…±ç¼©è¿›
            if sample_lines:
                try:
                    # æ‰¾åˆ°å®šä¹‰è¡Œçš„ç¼©è¿›ï¼ˆç¬¬ä¸€è¡Œï¼‰
                    indent = len(sample_lines[0]) - len(sample_lines[0].lstrip())
                    # å¯¹åç»­è¡Œåº”ç”¨ç›¸åŒçš„ç¼©è¿›ç§»é™¤
                    func_info["sample"] = [line[indent:].rstrip() for line in sample_lines]
                except IndexError:
                    pass # æ— æ³•è·å–ç¼©è¿›

        # Docstring é‡‡æ ·ä¸è§£æ (éœ€æ±‚3b, 4)
        docstring = ast.get_docstring(node)
        if docstring and len(docstring) > 10:
            params_from_doc = self._parse_docstring_params(docstring)
            doc_len = min(len(docstring), self.args.max_docstring_len)

            doc_entry = {
                "t": "md" if self.current_class else "fn",
                "n": node.name, 
                "doc": docstring[:doc_len] + ("..." if doc_len < len(docstring) else "")
            }
            if params_from_doc:
                doc_entry["doc_p"] = params_from_doc
            self.info["ds"].append(doc_entry)

        if self.current_class:
            self.info["md"].setdefault(self.current_class, []).append(func_info)
        else:
            self.info["fn"].append(func_info)

    def visit_FunctionDef(self, node):
        self._process_function_or_method(node)
        self.generic_visit(node)

    def visit_AsyncFunctionDef(self, node):
        self._process_function_or_method(node)
        self.generic_visit(node)

    # --- Assign (æ–°å¢å¸¸é‡/å˜é‡ä¿¡æ¯ - éœ€æ±‚1) ---

    def visit_Assign(self, node):
        # å¦‚æœåœ¨ ClassDef å¤– (æ¨¡å—çº§å˜é‡/å¸¸é‡)
        if self.current_class is None:
            value_repr = self._get_annotation(node.value, max_len=self.args.max_assign_len) 
            
            for target in node.targets:
                if isinstance(target, ast.Name):
                    name = target.id
                    is_constant = name.isupper() and (name.replace('_', '').isalnum()) # å¯å‘å¼åˆ¤æ–­
                    
                    self.info["cv"].append({
                        "n": name, 
                        "ln": target.lineno,
                        "const": is_constant,
                        "val": value_repr, 
                    })
        
        # æ³¨æ„: ç±»å±æ€§çš„æå–å·²ç§»è‡³ visit_ClassDef ä¸­ï¼Œä»¥é¿å…åœ¨æ–¹æ³•å†…èµ‹å€¼ä¹Ÿè¢«è¯¯åˆ¤ä¸ºç±»å±æ€§ã€‚
        self.generic_visit(node)
        
    # --- æ·±åº¦åˆ†æï¼šI/O, IPC, Error Handling (V2.0 ä¿ç•™) ---
    
    def visit_Call(self, node):
        call_name = self._get_name(node.func)
        for keyword, category in IO_CALLS.items():
            if keyword in call_name:
                self.info["stat"]["io"][category] += 1
        self.generic_visit(node)
        
    def visit_Try(self, node):
        self.info["stat"]["err"]["total"] += 1
        for handler in node.handlers:
            if handler.type is None or self._get_name(handler.type) in ["Exception", "BaseException"]:
                self.info["stat"]["err"]["generic"] += 1
                if "generic_excepts" not in self.info["stat"]["err"]:
                     self.info["stat"]["err"]["generic_excepts"] = []
                self.info["stat"]["err"]["generic_excepts"].append(handler.lineno)
        self.generic_visit(node)

    # --- è¾…åŠ©æ–¹æ³• (V3.0 ä¼˜åŒ– `_get_annotation` çš„å€¼æ•è·) ---

    def _get_name(self, node):
        if isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            base = self._get_name(node.value)
            return "{}.{}".format(base, node.attr) if base else node.attr
        elif isinstance(node, ast.Call):
            return self._get_name(node.func)
        return ""

    def _get_annotation(self, node, max_len=50):
        if node is None:
            return None
        
        # å°è¯•ä½¿ç”¨ ast.unparse (Python 3.9+) è·å–è¡¨è¾¾å¼çš„å­—ç¬¦ä¸²è¡¨ç¤º
        if hasattr(ast, "unparse"):
            try:
                representation = ast.unparse(node).strip()
                if len(representation) > max_len:
                    return representation[:max_len] + "..."
                return representation
            except Exception:
                # Fallback to type names if unparse fails
                pass 

        if isinstance(node, ast.Name):
            return node.id
        if isinstance(node, ast.Constant):
            # æ•è·å­—é¢é‡ç±»å‹å’Œå€¼
            val = text_type(node.value)
            type_name = type(node.value).__name__
            # é™åˆ¶ val çš„é•¿åº¦ä»¥é¿å…è¶…é•¿ token
            if len(val) > max_len:
                val = val[:max_len] + "..."
            return f"{type_name}({val})"
        if isinstance(node, ast.Subscript):
            value_name = self._get_name(node.value)
            return "{}[...]".format(value_name or "Subscript")
        
        return text_type(node)


# --- è§£æå…¥å£å‡½æ•° (é€‚é… V3.0) ---

def extract_python_semantics(filepath, args):
    """å¯¹å•ä¸ª Python æ–‡ä»¶åš AST è¯­ä¹‰æå– (V3.0)"""
    info = {
        "im": [], "fim": {}, "cl": [], "fn": [], "md": {}, "ds": [], "cv": [], "dc": [], "th": [], "cl_attr": [],
        "stat": {"async": 0, "th": 0, "io": {}, "err": {"total": 0, "generic": 0}},
    }
    content = ""
    content_lines = []

    try:
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()
        content = ensure_unicode(content)
        # ç”¨äºä»£ç é‡‡æ ·ï¼Œä¿ç•™æ¢è¡Œç¬¦
        content_lines = content.splitlines(keepends=True) 
        # AST è¡Œå·æ˜¯ 1-basedï¼Œä½† for å¾ªç¯ä¸­è·å– start_line: sample_end å·²ç»è°ƒæ•´ï¼Œè¿™é‡Œç”¨ splitlines() ä¸ä¿ç•™æ¢è¡Œï¼Œæ–¹ä¾¿å¤„ç†ç¼©è¿›
        content_lines = content.splitlines() 
    except Exception:
        return info

    try:
        # Python 3.8+ æ”¯æŒ end_lineno å’Œ end_col_offset
        tree = ast.parse(content)
        extractor = PythonSemanticExtractor(args)
        extractor.set_content_lines(content_lines)
        extractor.visit(tree)
        info.update(extractor.info)
        info["stat"]["io"] = dict(info["stat"]["io"])
    except SyntaxError:
        # AST è§£æå¤±è´¥æ—¶ï¼Œå›é€€åˆ°æ­£åˆ™
        print(f"âš ï¸ è¯­æ³•é”™è¯¯ï¼Œæ–‡ä»¶ {filepath} ä½¿ç”¨æ­£åˆ™å…œåº•ã€‚")
        return extract_python_semantics_fallback(content)
    except Exception as e:
        print(f"âŒ ä¸¥é‡é”™è¯¯ï¼šè§£æ {filepath} å¤±è´¥ï¼š{e}ã€‚ä½¿ç”¨æ­£åˆ™å…œåº•ã€‚")
        return extract_python_semantics_fallback(content)

    return info


def extract_python_semantics_fallback(content):
    """AST è§£æå¤±è´¥æ—¶çš„å…œåº•ï¼šåŸºäºæ­£åˆ™çš„ç²—ç²’åº¦æå–"""
    # ... (V2.0 é€»è¾‘ä¿æŒä¸å˜, æ— æ³•æä¾› V3.0 æ·±åº¦ä¿¡æ¯) ...
    info = {
        "im": [], "fim": {}, "cl": [], "fn": [], "md": {}, "ds": [], "cv": [], "dc": [], "th": [], "cl_attr": [],
        "stat": {"async": 0, "th": 0, "io": {}, "err": {"total": 0, "generic": 0}},
    }

    # æ­£åˆ™æå–: ä»…æå–åç§°å’Œå‚æ•° (æ—  CCN/ln/val/sample)
    # è¿™é‡Œæˆ‘ä»¬å°è¯•æ¨¡æ‹Ÿè¡Œå·ï¼Œä½†åªèƒ½æ˜¯ç²—ç•¥çš„è¿‘ä¼¼
    content_lines = content.splitlines()
    
    # Imports
    for i, line in enumerate(content_lines):
        match_import = re.match(r"^\s*import\s+([\w\.]+)", line)
        if match_import:
            info["im"].append({"n": match_import.group(1), "ln": i + 1})
        
        match_from_import = re.match(r"^\s*from\s+([\w\.]+)\s+import\s+(.+)", line)
        if match_from_import:
            module, items_str = match_from_import.groups()
            items_list = [{"n": item.strip(), "ln": i + 1} for item in items_str.split(",")]
            info["fim"].setdefault(module, []).extend(items_list)

    # Classes and Functions (éœ€è¦æ›´å¤æ‚çš„æ­£åˆ™æ¥è·å–å‡†ç¡®çš„è¡Œå·å’Œå‚æ•°)
    for i, line in enumerate(content_lines):
        match_class = re.match(r"^\s*class\s+(\w+)(?:\(([^)]*)\))?:", line)
        if match_class:
            name, bases = match_class.groups()
            info["cl"].append({"n": name, "ln": i + 1, "bs": [b.strip() for b in bases.split(",")] if bases else [], "attrs": []})
        
        match_func = re.match(r"^\s*(async\s+)?def\s+(\w+)\s*\(([^)]*)\)", line)
        if match_func:
            is_async, name, args_str = match_func.groups()
            args = [a.strip() for a in args_str.split(",")] if args_str else []
            func_info = {"n": name, "ln": i + 1, "cx": 1, "args": args, "async": bool(is_async)}
            info["fn"].append(func_info)
            if is_async: info["stat"]["async"] += 1
            
    # æ¸…ç† fn/cl åˆ—è¡¨ä¸­çš„é‡å¤é¡¹ï¼ˆæ­£åˆ™å¯èƒ½åœ¨å¤šè¡ŒåŒ¹é…ä¸­å‡ºé”™ï¼Œè™½ç„¶è¿™é‡Œæ˜¯å•è¡ŒåŒ¹é…ï¼‰
    # ç”±äºæ˜¯å…œåº•ï¼Œæˆ‘ä»¬æ¥å—å…¶ç²—ç³™æ€§ã€‚
    
    return info


# --- ä¾èµ–å›¾ç»˜åˆ¶ (ä¿ç•™ V2.0 é€»è¾‘) ---

def generate_dependency_graph(import_graph, output_path_base):
    # ... (V2.0 é€»è¾‘ä¿æŒä¸å˜) ...
    if not CAN_GRAPH:
        return

    G = nx.DiGraph()

    for module, deps in import_graph.items():
        if not deps:
            G.add_node(module)
        for dep in deps:
            G.add_edge(module, dep)

    if len(G.nodes) == 0:
        print("â„¹ï¸  ä¾èµ–å›¾ä¸ºç©ºï¼Œè·³è¿‡ç”Ÿæˆã€‚")
        return

    try:
        k = 1.0 / max(len(G.nodes), 1) ** 0.5
        pos = nx.spring_layout(G, k=k, iterations=80)
    except Exception:
        pos = nx.spring_layout(G)

    base_size = max(8, min(20, len(G.nodes) * 0.4))
    plt.figure(figsize=(base_size, base_size * 0.75))

    nx.draw_networkx_nodes(G, pos, node_size=800)
    nx.draw_networkx_edges(G, pos, arrows=True, arrowstyle="-|>", arrowsize=12)
    nx.draw_networkx_labels(G, pos, font_size=8)

    plt.axis("off")
    png_path = output_path_base + ".png"
    svg_path = output_path_base + ".svg"

    try:
        plt.tight_layout()
        plt.savefig(png_path, dpi=150)
        plt.savefig(svg_path, dpi=150)
        plt.close()
        print("âœ… Dependency graph generated: {}, {}".format(png_path, svg_path))
    except Exception as e:
        print("âš ï¸  ä¾èµ–å›¾ä¿å­˜å¤±è´¥: {}".format(ensure_unicode(str(e))))
        plt.close()


# --- é¡¹ç›®éå† & æ‘˜è¦ç”Ÿæˆ (V3.0) ---

CONFIG_FILES = [
    "requirements.txt", "setup.py", "pyproject.toml",
    "Pipfile", "poetry.lock", "tox.ini",
]

def rglob_py_files(root_path):
    # ... (V2.0 é€»è¾‘ä¿æŒä¸å˜) ...
    py_files = []
    root_path = ensure_unicode(root_path)
    for dirpath, dirnames, filenames in os.walk(root_path):
        dirpath = ensure_unicode(dirpath)
        dirnames[:] = [
            ensure_unicode(d) for d in dirnames
            if not ensure_unicode(d).startswith(".") and d not in ["__pycache__", "venv", "env", ".venv", "node_modules", "build", "dist", ".tox"]
        ]
        for filename in filenames:
            filename = ensure_unicode(filename)
            if filename.endswith(".py") and not filename.startswith("."):
                py_files.append(os.path.join(dirpath, filename))
    return py_files


def generate_semantic_digest(repo_path, output_path, args):
    """ç”Ÿæˆ Python é¡¹ç›®çš„è¯­ä¹‰æ‘˜è¦ï¼ˆV3.0ï¼‰"""
    repo_path = ensure_unicode(os.path.abspath(repo_path))
    ignored = get_gitignored_files(repo_path)

    digest = {
        "root": repo_path,
        "type": "python",
        "files": [],                       
        "modules": defaultdict(
            lambda: {
                "f": [],                    
                "im": [],                   # list of dicts (name, ln)
                "fim": defaultdict(list),   # dict of lists (name, ln)
                "cl": [],                   
                "fn": [],                   
                "md": {},                   
                "dc": set(),                
                "ds": [],                   
                "cv": [],                   # module vars/consts
                "cl_attr": [],              # class attributes
                "stat": {},                 
            }
        ),
        "deps": defaultdict(set),           
        "sum": {},                          
    }
    
    found_config_files = [f for f in CONFIG_FILES if os.path.exists(os.path.join(repo_path, f))]
    all_imports = set()
    total_ccn = 0
    
    for path in rglob_py_files(repo_path):
        path = ensure_unicode(path)
        rel_path = ensure_unicode(os.path.relpath(path, repo_path))

        if any(skip in rel_path for skip in ["test_", "_test.py", "/tests/", "/venv/", "/env/", "/.venv/", "/build/", "/dist/", "/__pycache__/", "/site-packages/"]):
            continue
        if rel_path in ignored:
            continue

        semantics = extract_python_semantics(path, args) # ä¼ å…¥ args

        module_parts = rel_path.replace(".py", "").replace(os.sep, ".").split(".")
        if module_parts and module_parts[-1] == "__init__":
            module_parts = module_parts[:-1]
        module_name = ".".join(module_parts) if module_parts else "root"

        mod_entry = digest["modules"][module_name]
        mod_entry["f"].append(rel_path)
        digest["files"].append(rel_path)

        # èšåˆç»“æ„
        mod_entry["im"].extend(semantics.get("im", []))
        mod_entry["cv"].extend(semantics.get("cv", []))
        mod_entry["cl_attr"].extend(semantics.get("cl_attr", []))
        
        for imp in semantics.get("im", []):
            all_imports.add(imp["n"])
        
        for module, items in semantics.get("fim", {}).items():
            mod_entry["fim"][module].extend(items)
            all_imports.add(module)
        
        mod_entry["cl"].extend(semantics.get("cl", []))
        mod_entry["fn"].extend(semantics.get("fn", []))
        for class_name, methods in semantics.get("md", {}).items():
            mod_entry["md"].setdefault(class_name, []).extend(methods)
            
        mod_entry["dc"].update(semantics.get("dc", []))
        mod_entry["ds"].extend(semantics.get("ds", []))
        
        mod_entry["stat"].update(semantics.get("stat", {}))
        
        # ç´¯åŠ  CCN
        total_ccn += sum(f.get("cx", 1) for f in semantics.get("fn", []))
        total_ccn += sum(m.get("cx", 1) for methods in semantics.get("md", {}).values() for m in methods)

        # æ„å»ºä¾èµ–å›¾ (ä½¿ç”¨å¯¼å…¥åï¼Œä¸ä½¿ç”¨è¡Œå·)
        for imp in semantics.get("im", []):
            # è¿‡æ»¤æ‰å¸¸è§çš„æ ‡å‡†åº“ï¼ˆå®ƒä»¬é€šå¸¸ä¸æ„æˆé¡¹ç›®å†…éƒ¨ä¾èµ–å›¾ï¼‰
            if not imp["n"].startswith(("sys", "os", "re", "json", "time", "datetime", "logging", "collections", "io", "abc", "math", "random", "unittest", "zipfile")):
                digest["deps"][module_name].add(imp["n"])
        for module, _items in semantics.get("fim", {}).items():
            if module and not module.startswith(("sys", "os", "re", "json", "time", "datetime", "logging", "collections", "io", "abc", "math", "random", "unittest", "zipfile")):
                digest["deps"][module_name].add(module)


    # --- æ”¶å°¾ï¼šæ¸…ç†ã€èšåˆã€ç”Ÿæˆé¡¹ç›®æ€»ç»“ (V3.0) ---

    total_functions = 0
    total_mod_ccn = 0
    
    # 1. æ¨¡å—çº§æ¸…ç†å’Œæ€»ç»“
    for module_name, data in digest["modules"].items():
        data["im"] = sorted(data["im"], key=lambda x: x["n"])
        data["dc"] = sorted(list(data["dc"]))

        # ç²¾ç®€ docstrings
        unique_docs = []
        seen = set()
        for doc in data["ds"]:
            key = (doc.get("t"), doc.get("n"))
            if key not in seen:
                seen.add(key)
                unique_docs.append(doc)
        data["ds"] = unique_docs[:5]

        # CCN Metrics (ç”¨äº sm æ‘˜è¦ - éœ€æ±‚5)
        all_cx = [f.get("cx", 1) for f in data["fn"]]
        all_cx.extend(m.get("cx", 1) for methods in data["md"].values() for m in methods)
        
        count_functions = len(data["fn"]) + sum(len(m) for m in data["md"].values())
        total_functions += count_functions
        
        total_module_ccn = sum(all_cx)
        total_mod_ccn += total_module_ccn
        
        avg_ccn = round(total_module_ccn / max(count_functions, 1), 1)
        high_ccn_count = sum(1 for cx in all_cx if cx >= args.ccn_threshold)

        # æ¨¡å—çº§ç®€çŸ­ summary (sm) - V3.0
        summary_parts = []
        if data["cl"]: summary_parts.append("CLS:{}".format(len(data["cl"])))
        if count_functions > 0: 
            summary_parts.append("FN:{}".format(count_functions))
            summary_parts.append("AVG_CX:{}".format(avg_ccn)) # éœ€æ±‚5
            if high_ccn_count > 0:
                 summary_parts.append("HIGH_CX:{}".format(high_ccn_count)) # éœ€æ±‚5
        if data["stat"].get("async"): summary_parts.append("ASYNC")
        if data["stat"].get("th"): summary_parts.append("TH")
        if any(v > 0 for v in data["stat"].get("io", {}).values()): summary_parts.append("IO/IPC")
        if data["stat"].get("err", {}).get("generic") > 0: summary_parts.append("GenericErr:{}".format(data["stat"]["err"]["generic"]))

        data["sm"] = "ï¼›".join(summary_parts) if summary_parts else "Python Module"

    digest["modules"] = dict(digest["modules"])

    # 2. ä¾èµ–å›¾ dict åŒ–
    dep_graph_dict = {}
    for mod, deps in digest["deps"].items():
        dep_graph_dict[mod] = sorted(list(deps))
    digest["deps"] = dep_graph_dict

    # 3. é¡¹ç›®çº§ summary (sum)
    total_modules = len(digest["modules"])
    total_classes = sum(len(m["cl"]) for m in digest["modules"].values())

    std_libs = {"os", "sys", "re", "json", "time", "logging", "datetime", "abc", "collections", "io", "math", "random", "unittest", "zipfile"}
    project_pkgs = set(digest["modules"].keys())
    # ç­›é€‰å‡ºéæ ‡å‡†åº“ä¸”éé¡¹ç›®å†…éƒ¨æ¨¡å—çš„å¯¼å…¥ï¼Œä½œä¸ºæŠ€æœ¯æ ˆ
    tech_stack = sorted(list((all_imports - std_libs) - project_pkgs))

    digest["sum"] = {
        "mod_count": total_modules,
        "cl_count": total_classes,
        "fn_count": total_functions,
        "file_count": len(digest["files"]),
        "total_ccn": total_mod_ccn,
        "config_files": found_config_files, 
        "tech_stack": tech_stack[:10], 
        "has_async": any(m.get("stat", {}).get("async") > 0 for m in digest["modules"].values()),
        # æ³¨æ„: æ— æ³•å‡†ç¡®ç»Ÿè®¡ type hintsï¼Œä¿æŒåŸå­—æ®µä½†ä¾èµ– AST ç»“æ„ (è¿™é‡Œç®€å•ä¿æŒ V2.0 é€»è¾‘)
        "uses_type_hints": False, 
    }

    output_path = ensure_unicode(output_path)
    try:
        with open(output_path, "w", encoding="utf-8") as f:
            yaml_content = yaml.dump(
                digest,
                allow_unicode=True,
                default_flow_style=False,
                sort_keys=False,
                width=120,
            )
            f.write(yaml_content)

        print("âœ… Semantic project digest generated: {}".format(output_path))
        print(
            "   ğŸ“Š Stats (V3.0): {} modules, {} classes, {} functions, Total CCN: {}".format(
                total_modules, total_classes, total_functions, total_mod_ccn
            )
        )

        try:
            graph_base = output_path.replace(".yaml", "_dependency_graph")
            generate_dependency_graph(digest["deps"], graph_base)
        except Exception as e:
            print("âš ï¸  ç”Ÿæˆä¾èµ–å›¾æ—¶å‡ºé”™: {}".format(ensure_unicode(str(e))))
            
    except Exception as e:
        print("âŒ å†™å…¥è¾“å‡ºæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {}".format(ensure_unicode(str(e))))


# --- CLI å…¥å£ (V3.0 æ–°å¢å‚æ•°) ---

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description=(
            "ç”Ÿæˆ Python é¡¹ç›®çš„è¯­ä¹‰æ‘˜è¦ï¼ˆYAMLï¼‰å’Œä¾èµ–å…³ç³»å›¾ï¼ˆPNG/SVGï¼‰ï¼Œ"
            "ç”¨äºå¤§æ¨¡å‹ç†è§£é¡¹ç›®ç»“æ„ & å¤§å¹…å‡å°‘ token (V3.0)."
        ),
        epilog=(
            "ç¤ºä¾‹:\n"
            "  python3.9 py-semantic-snapshot.py ./project -o digest.yaml --max-doc-len 100 --ccn-threshold 10"
        ),
    )
    parser.add_argument("repo_path", help="æœ¬åœ° Python é¡¹ç›®è·¯å¾„")
    parser.add_argument(
        "-o",
        "--output",
        default="python_semantic_digest.yaml",
        help="è¾“å‡º YAML æ–‡ä»¶è·¯å¾„",
    )
    # V3.0 æ–°å¢ Token æ§åˆ¶å’Œé‡‡æ ·å‚æ•° (éœ€æ±‚4, 6)
    parser.add_argument(
        "--max-doc-len",
        type=int,
        default=200,
        dest="max_docstring_len",
        help="Docstring é‡‡æ ·æœ€å¤§é•¿åº¦ (Token ä¼˜åŒ–).",
    )
    parser.add_argument(
        "--max-args-len",
        type=int,
        default=5,
        dest="max_args_len",
        help="å‡½æ•°å‚æ•°åˆ—è¡¨æœ€å¤§æ•°é‡ï¼Œè¶…å‡ºåˆ™æˆªæ–­ (Token ä¼˜åŒ–).",
    )
    parser.add_argument(
        "--max-assign-len",
        type=int,
        default=30,
        dest="max_assign_len",
        help="å¸¸é‡/å˜é‡/å±æ€§åˆå§‹å€¼è¡¨è¾¾å¼çš„æœ€å¤§é•¿åº¦ (Token ä¼˜åŒ–).",
    )
    parser.add_argument(
        "--ccn-threshold",
        type=int,
        default=10,
        dest="ccn_threshold",
        help="CCN é˜ˆå€¼ã€‚é«˜äºæ­¤å€¼çš„å‡½æ•°ï¼Œå…¶ä»£ç ç‰‡æ®µå°†è¢«é‡‡æ · (éœ€æ±‚6) å¹¶åœ¨æ‘˜è¦ä¸­æ ‡è®° (éœ€æ±‚5).",
    )
    parser.add_argument(
        "--code-sample-lines",
        type=int,
        default=5,
        dest="code_sample_lines",
        help="é«˜ CCN å‡½æ•°çš„ä»£ç é‡‡æ ·è¡Œæ•° (éœ€æ±‚6).",
    )

    args = parser.parse_args()

    if not os.path.exists(args.repo_path):
        print("âŒ Error: Path '{}' does not exist".format(args.repo_path))
        sys.exit(1)

    generate_semantic_digest(args.repo_path, args.output, args)
